# Pyspark  
Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark.  
**for installation:** At first, be sure you have installed Java and Scala and Apache Spark. Then you can easily install this library by "pip install pyspark".  
**SparkContext** is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.  
