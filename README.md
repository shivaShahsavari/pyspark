# Pyspark  
Apache Spark is written in Scala programming language. To support Python with Spark, Apache Spark Community released a tool, PySpark.  
**for installation:** At first, be sure you have installed Java and Scala and Apache Spark. Then easily by code "pip install pyspark" you can install this library.  
**SparkContext** is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.  
